# aixi_booster
symbolic regression for sequence prediction

AIXI-like system. Uses a Brainfuck-like language to write a code that makes Y_train sequence from X_train sequence. The main mechanic of code generation is a genetic algorythm.
The fitness function looks like this:
1) Y_train_normed = Y_train/(mean(abs(Y_train)))
2) X_train[i, :] goes into the 3-tapes (0th, 1st, 2nd, input is 0th) Turing Machine generated by our Brainfuck-like language
3) Turing Machine makes tact_count (10 by default) iterations 
4) Numbers in the 1st tape are interpreted as output of Turing Machine (y_pred_turing)
5) Make steps 2-4 for all X_train strings
6) Than we concat X_train strings and y_pred_turing strings - this is named "linear input"
7) Than we make an ML-model that has a linear input as it's input and Y_train as output. ML-model may be Ridge or boosting.
8) Than we interpetate a model output as Y_predicted_normed
9) fitness = - MSE(Y_train_normed, Y_predicted_normed) - penalty_for_complexity
10) penalty_for_complexity = regularization_coefficient*count_non_empty_strings_in_Brainfuck_code/maximum_count_strings_available_in_Brainfuck_code

Than we make boosting of such models. And a weighted forest of boostings.
